use pretrain model ./STDCNet813M_73.91.tar
epoch 0, lr 0.010000, lr_d 0.000100:  63% 1575/2499 [11:56<07:00,  2.20it/s, loss=1.366080]
loss for train : 1.953479
epoch 1, lr 0.009820, lr_d 0.000098:  63% 1575/2499 [08:56<05:14,  2.94it/s, loss=0.938614]
loss for train : 1.052506
epoch 2, lr 0.009639, lr_d 0.000096:  63% 1575/2499 [08:54<05:13,  2.95it/s, loss=0.725276]
loss for train : 0.893261
epoch 3, lr 0.009458, lr_d 0.000095:  63% 1575/2499 [08:51<05:11,  2.96it/s, loss=1.123100]
loss for train : 0.790874
epoch 4, lr 0.009277, lr_d 0.000093:  63% 1575/2499 [08:54<05:13,  2.94it/s, loss=0.723320]
loss for train : 0.715889
epoch 5, lr 0.009095, lr_d 0.000091:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.695974]
loss for train : 0.616691
epoch 6, lr 0.008913, lr_d 0.000089:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.499245]
loss for train : 0.602249
epoch 7, lr 0.008731, lr_d 0.000087:  63% 1575/2499 [08:51<05:12,  2.96it/s, loss=0.430012]
loss for train : 0.552412
epoch 8, lr 0.008548, lr_d 0.000085:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.497223]
loss for train : 0.510383
epoch 9, lr 0.008364, lr_d 0.000084:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.606215]
loss for train : 0.482147
epoch 10, lr 0.008181, lr_d 0.000082:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.569225]
loss for train : 0.475461
epoch 11, lr 0.007996, lr_d 0.000080:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.388925]
loss for train : 0.447354
epoch 12, lr 0.007811, lr_d 0.000078:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.465163]
loss for train : 0.423828
epoch 13, lr 0.007626, lr_d 0.000076:  63% 1575/2499 [08:51<05:12,  2.96it/s, loss=0.378788]
loss for train : 0.426609
epoch 14, lr 0.007440, lr_d 0.000074:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.329850]
loss for train : 0.401379
epoch 15, lr 0.007254, lr_d 0.000073:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.434888]
loss for train : 0.383663
epoch 16, lr 0.007067, lr_d 0.000071:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.486882]
loss for train : 0.379693
epoch 17, lr 0.006880, lr_d 0.000069:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.374516]
loss for train : 0.367011
epoch 18, lr 0.006692, lr_d 0.000067:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.348891]
loss for train : 0.358327
epoch 19, lr 0.006504, lr_d 0.000065:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.365079]
loss for train : 0.355404
epoch 20, lr 0.006314, lr_d 0.000063:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.380785]
loss for train : 0.345291
epoch 21, lr 0.006125, lr_d 0.000061:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.329558]
loss for train : 0.340068
epoch 22, lr 0.005934, lr_d 0.000059:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.257435]
loss for train : 0.334151
epoch 23, lr 0.005743, lr_d 0.000057:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.345312]
loss for train : 0.335686
epoch 24, lr 0.005551, lr_d 0.000056:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.300896]
loss for train : 0.325702
epoch 25, lr 0.005359, lr_d 0.000054:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.360647]
loss for train : 0.325910
epoch 26, lr 0.005166, lr_d 0.000052:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.365873]
loss for train : 0.317612
epoch 27, lr 0.004971, lr_d 0.000050:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.295490]
loss for train : 0.314521
epoch 28, lr 0.004776, lr_d 0.000048:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.287764]
loss for train : 0.313095
epoch 29, lr 0.004581, lr_d 0.000046:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.290042]
loss for train : 0.310283
epoch 30, lr 0.004384, lr_d 0.000044:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.331508]
loss for train : 0.310672
epoch 31, lr 0.004186, lr_d 0.000042:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.295114]
loss for train : 0.302859
epoch 32, lr 0.003987, lr_d 0.000040:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.305219]
loss for train : 0.302179
epoch 33, lr 0.003787, lr_d 0.000038:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.286558]
loss for train : 0.302272
epoch 34, lr 0.003586, lr_d 0.000036:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.280930]
loss for train : 0.296530
epoch 35, lr 0.003384, lr_d 0.000034:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.371864]
loss for train : 0.295622
epoch 36, lr 0.003180, lr_d 0.000032:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.312343]
loss for train : 0.292100
epoch 37, lr 0.002975, lr_d 0.000030:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.340637]
loss for train : 0.289714
epoch 38, lr 0.002768, lr_d 0.000028:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.330786]
loss for train : 0.292013
epoch 39, lr 0.002560, lr_d 0.000026:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.350372]
loss for train : 0.287578
epoch 40, lr 0.002349, lr_d 0.000023:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.245889]
loss for train : 0.286937
epoch 41, lr 0.002137, lr_d 0.000021:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.288899]
loss for train : 0.286613
epoch 42, lr 0.001922, lr_d 0.000019:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.299039]
loss for train : 0.284301
epoch 43, lr 0.001704, lr_d 0.000017:  63% 1575/2499 [08:53<05:13,  2.95it/s, loss=0.256989]
loss for train : 0.283746
epoch 44, lr 0.001483, lr_d 0.000015:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.281873]
loss for train : 0.281389
epoch 45, lr 0.001259, lr_d 0.000013:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.275201]
loss for train : 0.280835
epoch 46, lr 0.001030, lr_d 0.000010:  63% 1575/2499 [08:52<05:12,  2.96it/s, loss=0.289678]
loss for train : 0.281129
epoch 47, lr 0.000795, lr_d 0.000008:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.326644]
loss for train : 0.281605
epoch 48, lr 0.000552, lr_d 0.000006:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.220051]
loss for train : 0.279327
epoch 49, lr 0.000296, lr_d 0.000003:  63% 1575/2499 [08:53<05:12,  2.95it/s, loss=0.310604]
loss for train : 0.281418
start val!
precision per pixel for test: 0.663
mIoU for validation: 0.258
mIoU per class: [0.81015303 0.09938436 0.72914947 0.08296185 0.07340487 0.21740073
 0.115107   0.09668478 0.56255042 0.04480372 0.70424215 0.34949003
 0.04376787 0.66321219 0.0932354  0.13740435 0.0575015  0.01489066
 0.        ]