use pretrain model ./STDCNet813M_73.91.tar
epoch 0, lr 0.010000, lr_d 0.000100:  63% 1575/2499 [03:57<02:19,  6.64it/s, loss=0.915044]
loss for train : 1.981157
epoch 1, lr 0.009820, lr_d 0.000098:  63% 1575/2499 [03:57<02:19,  6.64it/s, loss=0.846625]
loss for train : 1.102873
epoch 2, lr 0.009639, lr_d 0.000096:  63% 1575/2499 [03:58<02:19,  6.62it/s, loss=0.918106]
loss for train : 0.947814
epoch 3, lr 0.009458, lr_d 0.000095:  63% 1575/2499 [03:58<02:20,  6.60it/s, loss=0.715576]
loss for train : 0.833955
epoch 4, lr 0.009277, lr_d 0.000093:  63% 1575/2499 [03:57<02:19,  6.62it/s, loss=0.810818]
loss for train : 0.789728
epoch 5, lr 0.009095, lr_d 0.000091:  63% 1575/2499 [03:59<02:20,  6.58it/s, loss=0.754617]
loss for train : 0.726192
epoch 6, lr 0.008913, lr_d 0.000089:  63% 1575/2499 [03:58<02:19,  6.60it/s, loss=0.721042]
loss for train : 0.669417
epoch 7, lr 0.008731, lr_d 0.000087:  63% 1575/2499 [03:57<02:19,  6.62it/s, loss=0.727156]
loss for train : 0.647137
epoch 8, lr 0.008548, lr_d 0.000085:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.647856]
loss for train : 0.597026
epoch 9, lr 0.008364, lr_d 0.000084:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.538112]
loss for train : 0.574729
epoch 10, lr 0.008181, lr_d 0.000082:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.558077]
loss for train : 0.560698
epoch 11, lr 0.007996, lr_d 0.000080:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.542342]
loss for train : 0.535048
epoch 12, lr 0.007811, lr_d 0.000078:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.452545]
loss for train : 0.534959
epoch 13, lr 0.007626, lr_d 0.000076:  63% 1575/2499 [03:58<02:19,  6.62it/s, loss=0.549476]
loss for train : 0.515974
epoch 14, lr 0.007440, lr_d 0.000074:  63% 1575/2499 [03:57<02:19,  6.62it/s, loss=0.441368]
loss for train : 0.500600
epoch 15, lr 0.007254, lr_d 0.000073:  63% 1575/2499 [03:57<02:19,  6.63it/s, loss=0.487310]
loss for train : 0.485689
epoch 16, lr 0.007067, lr_d 0.000071:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.527582]
loss for train : 0.477230
epoch 17, lr 0.006880, lr_d 0.000069:  63% 1575/2499 [03:57<02:19,  6.64it/s, loss=0.467227]
loss for train : 0.469455
epoch 18, lr 0.006692, lr_d 0.000067:  63% 1575/2499 [03:58<02:19,  6.62it/s, loss=0.461125]
loss for train : 0.459963
epoch 19, lr 0.006504, lr_d 0.000065:  63% 1575/2499 [03:57<02:19,  6.62it/s, loss=0.494523]
loss for train : 0.455963
epoch 20, lr 0.006314, lr_d 0.000063:  63% 1575/2499 [03:57<02:19,  6.62it/s, loss=0.461199]
loss for train : 0.443535
epoch 21, lr 0.006125, lr_d 0.000061:  63% 1575/2499 [03:58<02:19,  6.62it/s, loss=0.526976]
loss for train : 0.448563
epoch 22, lr 0.005934, lr_d 0.000059:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.485807]
loss for train : 0.446443
epoch 23, lr 0.005743, lr_d 0.000057:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.418854]
loss for train : 0.433712
epoch 24, lr 0.005551, lr_d 0.000056:  63% 1575/2499 [03:57<02:19,  6.62it/s, loss=0.409062]
loss for train : 0.427393
epoch 25, lr 0.005359, lr_d 0.000054:  63% 1575/2499 [03:57<02:19,  6.64it/s, loss=0.433463]
loss for train : 0.423802
epoch 26, lr 0.005166, lr_d 0.000052:  63% 1575/2499 [03:57<02:19,  6.63it/s, loss=0.463045]
loss for train : 0.420705
epoch 27, lr 0.004971, lr_d 0.000050:  63% 1575/2499 [03:58<02:20,  6.59it/s, loss=0.573162]
loss for train : 0.414874
epoch 28, lr 0.004776, lr_d 0.000048:  63% 1575/2499 [04:00<02:21,  6.55it/s, loss=0.380401]
loss for train : 0.408423
epoch 29, lr 0.004581, lr_d 0.000046:  63% 1575/2499 [04:00<02:21,  6.55it/s, loss=0.421132]
loss for train : 0.404517
epoch 30, lr 0.004384, lr_d 0.000044:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.380584]
loss for train : 0.411414
epoch 31, lr 0.004186, lr_d 0.000042:  63% 1575/2499 [03:58<02:20,  6.59it/s, loss=0.444224]
loss for train : 0.404904
epoch 32, lr 0.003987, lr_d 0.000040:  63% 1575/2499 [03:58<02:20,  6.60it/s, loss=0.346867]
loss for train : 0.398832
epoch 33, lr 0.003787, lr_d 0.000038:  63% 1575/2499 [03:57<02:19,  6.62it/s, loss=0.370706]
loss for train : 0.395381
epoch 34, lr 0.003586, lr_d 0.000036:  63% 1575/2499 [03:57<02:19,  6.64it/s, loss=0.414656]
loss for train : 0.391120
epoch 35, lr 0.003384, lr_d 0.000034:  63% 1575/2499 [03:59<02:20,  6.58it/s, loss=0.412397]
loss for train : 0.390807
epoch 36, lr 0.003180, lr_d 0.000032:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.450334]
loss for train : 0.389461
epoch 37, lr 0.002975, lr_d 0.000030:  63% 1575/2499 [03:58<02:20,  6.60it/s, loss=0.367600]
loss for train : 0.385033
epoch 38, lr 0.002768, lr_d 0.000028:  63% 1575/2499 [03:58<02:20,  6.60it/s, loss=0.402675]
loss for train : 0.387435
epoch 39, lr 0.002560, lr_d 0.000026:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.358339]
loss for train : 0.380748
epoch 40, lr 0.002349, lr_d 0.000023:  63% 1575/2499 [03:58<02:19,  6.60it/s, loss=0.376826]
loss for train : 0.379723
epoch 41, lr 0.002137, lr_d 0.000021:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.465487]
loss for train : 0.380073
epoch 42, lr 0.001922, lr_d 0.000019:  63% 1575/2499 [03:58<02:20,  6.59it/s, loss=0.430104]
loss for train : 0.380272
epoch 43, lr 0.001704, lr_d 0.000017:  63% 1575/2499 [03:58<02:20,  6.59it/s, loss=0.296731]
loss for train : 0.378607
epoch 44, lr 0.001483, lr_d 0.000015:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.346120]
loss for train : 0.375219
epoch 45, lr 0.001259, lr_d 0.000013:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.412889]
loss for train : 0.376448
epoch 46, lr 0.001030, lr_d 0.000010:  63% 1575/2499 [03:58<02:19,  6.61it/s, loss=0.343676]
loss for train : 0.372135
epoch 47, lr 0.000795, lr_d 0.000008:  63% 1575/2499 [03:57<02:19,  6.63it/s, loss=0.422927]
loss for train : 0.371262
epoch 48, lr 0.000552, lr_d 0.000006:  63% 1575/2499 [03:58<02:19,  6.62it/s, loss=0.363602]
loss for train : 0.371097
epoch 49, lr 0.000296, lr_d 0.000003:  63% 1575/2499 [03:58<02:19,  6.60it/s, loss=0.344779]
loss for train : 0.372173
start val!
precision per pixel for test: 0.648
mIoU for validation: 0.254
mIoU per class: [7.60287212e-01 2.19903131e-01 7.09363277e-01 6.93265857e-02
 2.38182784e-02 2.82303315e-01 4.97417622e-02 1.01581966e-01
 6.89238381e-01 6.59975474e-02 6.58909162e-01 3.09208648e-01
 3.58860396e-02 6.52481350e-01 6.22503220e-02 8.65698924e-02
 1.74186926e-02 2.61894708e-02 1.36904837e-05]